import torch
import torch.nn.functional as F
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from FFT_PSF import compute_psf, FFT_PSF_for_training, torch_apply_psf
from Aberation_cnn import AberrationCNN, AberrationLoss, PaperParams,AberrationLossV2
from Data_loader import create_synthetic_image, create_training_dataset
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import os
import datetime
from Extract_PSF import *

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # -------------------------- 1. æ–°å¢ï¼šåˆ›å»ºæœ€ä½³æ¨¡å‹ä¸“å±ä¿å­˜æ–‡ä»¶å¤¹ --------------------------
    # å®šä¹‰æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå¯è‡ªå®šä¹‰ï¼Œå¦‚"best_models"ï¼‰
    best_model_dir = "best_pre_correction_models"
    # è‹¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨åˆ™åˆ›å»ºï¼Œexist_ok=Trueé¿å…é‡å¤åˆ›å»ºæŠ¥é”™
    os.makedirs(best_model_dir, exist_ok=True)
    # ç”¨äºè®°å½•å·²ä¿å­˜çš„æœ€ä½³æ¨¡å‹ä¿¡æ¯ï¼šåˆ—è¡¨å…ƒç´ ä¸º (æŸå¤±å€¼, æ¨¡å‹æ–‡ä»¶è·¯å¾„)
    saved_best_models = []
    # è®¾å®šä¿ç•™çš„æœ€å¤§æ¨¡å‹æ•°é‡
    max_keep_models = 10

    # 2. å®šä¹‰æ³¢é•¿å’Œåƒå·®ç³»æ•°
    lambda_values = np.array([486, 587, 656]) * 1e-9
    aberration_coeffs = np.array([
        [0.4548, -0.0365, 2.0154, 0.9962],  # è“å…‰
        [0.4202, -0.0506, 1.6590, 0.8220],  # ç»¿å…‰
        [0.3905, -0.0509, 1.4816, 0.7348]  # çº¢å…‰
    ])
    # aberration_coeffs = np.array([
    #     [0.4548, -0.0218, 0.7161, 0.3540],  # è“å…‰
    #     [0.4202, -0.0302, 0.5895, 0.2921],  # ç»¿å…‰
    #     [0.3905, -0.0304, 0.5264, 0.2611]  # çº¢å…‰
    # ])

    # 3. è®¡ç®— PSF
    # PSF = compute_psf(lambda_values, aberration_coeffs,g=8.48e-3, visualize=False)
    PSF=extract_psf(file_path="PSF.txt",visual=False)
    
    # 4. åŠ è½½è®­ç»ƒ/éªŒè¯å›¾ç‰‡è·¯å¾„
    train_dir = "F:\\BaiduNetdiskDownload\\COCO_2014\\train2014\\"
    val_dir = "F:\\BaiduNetdiskDownload\\COCO_2014\\val2014\\"
    train_image_paths = [os.path.join(train_dir, img) for img in os.listdir(train_dir) if img.endswith('.jpg')]
    val_image_paths = [os.path.join(val_dir, img) for img in os.listdir(val_dir) if img.endswith('.jpg')]

    if not train_image_paths:
        print("No training images found. Using synthetic data.")
        train_image_paths = []
    if not val_image_paths:
        print("No validation images found. Using synthetic data.")
        val_image_paths = []

    # 5. åˆ›å»ºæ•°æ®é›†
    # æ­¤å¤„æ‰€å¾—åˆ°çš„train_clean_imageså’Œval_clean_imageséƒ½æ˜¯â€œå¹²å‡€å›¾åƒâ€çš„Pytorchå‘é‡åˆ—è¡¨
    # åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ æ˜¯â€œå•å¼ å¹²å‡€å›¾åƒâ€çš„å‘é‡(torch.Tensor)ï¼Œç»´åº¦ä¸º(C,H,W)ï¼Œæ•°å€¼èŒƒå›´å½’ä¸€åŒ–åˆ°[0,1]
    train_clean_images, _ = create_training_dataset(PSF, image_paths=train_image_paths,
                                                    synthetic_count=100 if not train_image_paths else 0, image_size=PaperParams.EI_SIZE[0],
                                                    device=device, max_images=5000)
    val_clean_images, _ = create_training_dataset(PSF, image_paths=val_image_paths,
                                                  synthetic_count=50 if not val_image_paths else 0, image_size=PaperParams.EI_SIZE[1],
                                                  device=device, max_images=200)

    # 6. å®šä¹‰ Dataset å’Œ DataLoader
    class PreCorrectionDataset(Dataset):
        def __init__(self, clean_images):
            self.clean_images = clean_images
        def __len__(self):
            return len(self.clean_images)
        def __getitem__(self, idx):
            return self.clean_images[idx]

    # å°†å¹²å‡€å›¾åƒå¼ é‡å°è£…åˆ°PreCorrectionDatasetä¸­ï¼Œé€šè¿‡DataLoaderæŒ‰æ‰¹æ¬¡è¾“å…¥æ¨¡å‹ï¼Œä½œä¸ºè®­ç»ƒ/éªŒè¯çš„â€œæ ‡ç­¾â€ï¼Œç”¨äºè®¡ç®—æ¨¡å‹é¢„æ ¡æ­£æ•ˆæœçš„æŸå¤±
    train_loader = DataLoader(PreCorrectionDataset(train_clean_images), batch_size=PaperParams.BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(PreCorrectionDataset(val_clean_images), batch_size=PaperParams.BATCH_SIZE, shuffle=False)

    # 7. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
    model = AberrationCNN().to(device)
    criterion = AberrationLossV2(omega=0.7,bright_weight=2.0).to(device)
    optimizer = optim.SGD(model.parameters(), lr=PaperParams.LR, momentum=0.9)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.8, patience=3)

    # 8. è®­ç»ƒå¾ªç¯ï¼ˆæ ¸å¿ƒï¼šæœ€ä½³æ¨¡å‹ä¿å­˜ä¸ç­›é€‰ï¼‰
    epochs = PaperParams.EPOCHS
    best_val_loss = float('inf')
    model.train()

    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        running_loss = 0.0
        for clean_batch in train_loader:
            clean_batch = clean_batch.to(device)
            optimizer.zero_grad()
            pre_corrected = model(clean_batch)
            pre_corrected = torch.clamp(pre_corrected, 0.0, 1.0)
            simulated = torch.zeros_like(pre_corrected)
            for i in range(pre_corrected.size(0)):
                simulated[i] = torch_apply_psf(PSF, pre_corrected[i])
            loss = criterion(simulated, clean_batch,pre_corrected)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            running_loss += loss.item()

        avg_train_loss = running_loss / len(train_loader)
        print(f"Epoch [{epoch + 1}/{epochs}], Train Average Loss: {avg_train_loss:.6f}")

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for val_batch in val_loader:
                val_batch = val_batch.to(device)
                pre_corrected_val = model(val_batch)
                simulated_val = torch.zeros_like(pre_corrected_val)
                for i in range(pre_corrected_val.size(0)):
                    simulated_val[i] = torch_apply_psf(PSF, pre_corrected_val[i])
                val_loss += criterion(simulated_val, val_batch).item()
        avg_val_loss = val_loss / len(val_loader)
        print(f"Epoch [{epoch + 1}/{epochs}], Val Average Loss: {avg_val_loss:.6f}")
        scheduler.step(avg_val_loss)

        # -------------------------- 2. æ ¸å¿ƒä¿®æ”¹ï¼šæœ€ä½³æ¨¡å‹ä¿å­˜ä¸ç­›é€‰ --------------------------
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            # ç”Ÿæˆå¸¦ã€Œæ—¶é—´+æŸå¤±å€¼ã€çš„æ–‡ä»¶åï¼ˆä¾¿äºè¯†åˆ«æ€§èƒ½ï¼‰
            current_time = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            # æ–‡ä»¶åæ ¼å¼ï¼šæ¨¡å‹å_æ—¶é—´_æŸå¤±å€¼.pthï¼ˆæŸå¤±å€¼ä¿ç•™4ä½å°æ•°ï¼Œé¿å…è¿‡é•¿ï¼‰
            model_filename = f"best_model_{current_time}_loss{avg_val_loss:.4f}.pth"
            # æ‹¼æ¥å®Œæ•´ä¿å­˜è·¯å¾„ï¼ˆä¸“å±æ–‡ä»¶å¤¹ + æ–‡ä»¶åï¼‰
            model_save_path = os.path.join(best_model_dir, model_filename)

            # ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹
            torch.save(model.state_dict(), model_save_path)
            print(f"âœ… æ–°æœ€ä½³æ¨¡å‹ä¿å­˜ï¼š{model_save_path}")

            # å°†æ–°æ¨¡å‹ä¿¡æ¯åŠ å…¥åˆ—è¡¨ï¼š(æŸå¤±å€¼, ä¿å­˜è·¯å¾„)
            saved_best_models.append( (avg_val_loss, model_save_path) )

            # -------------------------- 3. ç­›é€‰ï¼šä»…ä¿ç•™æœ€å¥½çš„10ä¸ªæ¨¡å‹ --------------------------
            # æŒ‰æŸå¤±å€¼å‡åºæ’åºï¼ˆæŸå¤±è¶Šå°ï¼Œæ¨¡å‹è¶Šå¥½ï¼Œæ’åœ¨å‰é¢ï¼‰
            saved_best_models.sort(key=lambda x: x[0])
            # è‹¥æ¨¡å‹æ•°é‡è¶…è¿‡10ä¸ªï¼Œåˆ é™¤è¶…å‡ºçš„æ—§æ¨¡å‹ï¼ˆä»ç¬¬11ä¸ªå¼€å§‹ï¼‰
            if len(saved_best_models) > max_keep_models:
                # è·å–éœ€è¦åˆ é™¤çš„æ¨¡å‹ï¼ˆæ’åºåç¬¬10ä¸ªä¹‹åçš„ï¼‰
                models_to_delete = saved_best_models[max_keep_models:]
                # æ›´æ–°åˆ—è¡¨ï¼šåªä¿ç•™å‰10ä¸ª
                saved_best_models = saved_best_models[:max_keep_models]

                # éå†åˆ é™¤è¶…å‡ºçš„æ¨¡å‹æ–‡ä»¶
                for loss_del, path_del in models_to_delete:
                    if os.path.exists(path_del):
                        os.remove(path_del)
                        print(f"ğŸ—‘ï¸ åˆ é™¤æ—§æ¨¡å‹ï¼ˆè¶…å‡º10ä¸ªï¼‰ï¼š{os.path.basename(path_del)} (æŸå¤±ï¼š{loss_del:.4f})")

    # 9. ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆå¯é€‰ï¼šä¹Ÿæ”¾å…¥ä¸“å±æ–‡ä»¶å¤¹ï¼‰
    final_time = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    final_model_path = os.path.join(best_model_dir, f"final_model_{final_time}.pth")
    torch.save(model.state_dict(), final_model_path)
    print(f"\nğŸ“¦ æœ€ç»ˆæ¨¡å‹ä¿å­˜ï¼š{final_model_path}")

    # 10. æµ‹è¯•ä¸å¯è§†åŒ–ï¼ˆä¿æŒåŸé€»è¾‘ä¸å˜ï¼‰
    model.eval()
    with torch.no_grad():
        test_img_path = "F:\\BaiduNetdiskDownload\\COCO_2014\\train2014\\COCO_train2014_000000000025.jpg" if os.path.exists(
            "F:\\BaiduNetdiskDownload\\COCO_2014\\train2014\\COCO_train2014_000000000025.jpg") else None
        if test_img_path:
            test_img = Image.open(test_img_path).convert("RGB").resize((PaperParams.EI_SIZE[0],PaperParams.EI_SIZE[1]))
            test_array = np.array(test_img).astype(np.float32) / 255.0
        else:
            test_array = create_synthetic_image(size=PaperParams.EI_SIZE[1])

        test_tensor = torch.from_numpy(test_array.transpose(2, 0, 1)).float().unsqueeze(0).to(device)
        pre_corrected_test = model(test_tensor)[0]
        pre_corrected_np = pre_corrected_test.detach().cpu().numpy().transpose(1, 2, 0)
        Image.fromarray((pre_corrected_np * 255).astype(np.uint8)).save("pre_corrected_EI.png")

        simulated_test = torch_apply_psf(PSF, pre_corrected_test)
        simulated_np = simulated_test.detach().cpu().numpy().transpose(1, 2, 0)

        plt.figure(figsize=(15, 5))
        plt.subplot(1, 3, 1)
        plt.imshow(test_array)
        plt.title("Original EI")
        plt.subplot(1, 3, 2)
        plt.imshow(pre_corrected_np)
        plt.title("Pre-corrected EI")
        plt.subplot(1, 3, 3)
        plt.imshow(simulated_np)
        plt.title("Simulated EI (after PSF)")
        plt.tight_layout()
        plt.show()

    print("æµ‹è¯•å®Œæˆï¼Œé¢„æ ¡æ­£ EI å·²ä¿å­˜ä¸º pre_corrected_EI.png")

    # 11. è®¡ç®—æµ‹è¯•æŸå¤±ï¼ˆä¿®å¤åŸé€»è¾‘ï¼‰
    loss_value, loss1_value = test()
    print(f"é¢„æ ¡æ­£åæ¨¡æ‹Ÿæˆåƒ Loss vs åŸå§‹ EI: {loss_value:.6f}")
    print(f"åŸå§‹ EI ç›´æ¥æ¨¡æ‹Ÿæˆåƒ Loss: {loss1_value:.6f}")


def test():
    # ä¿æŒåŸtestå‡½æ•°é€»è¾‘ä¸å˜ï¼Œä»…ä¿®å¤è¿”å›å€¼åŒ¹é…é—®é¢˜
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    pre_img_path = "pre_corrected_EI.png"
    ini_img_path = "F:\\BaiduNetdiskDownload\\COCO_2014\\train2014\\COCO_train2014_000000000025.jpg"

    if not os.path.exists(pre_img_path) or not os.path.exists(ini_img_path):
        print("Error: Pre-corrected EI or Original EI not found.")
        return float('inf'), float('inf')

    pre_img = Image.open(pre_img_path).convert("RGB").resize((104, 104))
    pre_array = np.array(pre_img).astype(np.float32) / 255.0
    ini_img = Image.open(ini_img_path).convert("RGB").resize((104, 104))
    ini_array = np.array(ini_img).astype(np.float32) / 255.0

    pre_tensor = torch.from_numpy(pre_array.transpose(2, 0, 1)).float().unsqueeze(0).to(device)
    ini_tensor = torch.from_numpy(ini_array.transpose(2, 0, 1)).float().unsqueeze(0).to(device)

    lambda_values = np.array([486, 587, 656]) * 1e-9
    aberration_coeffs = np.array([
        [0.4548, -0.0218, 0.7161, 0.3540],
        [0.4202, -0.0302, 0.5895, 0.2921],
        [0.3905, -0.0304, 0.5264, 0.2611]
    ])
    PSF = torch.from_numpy(compute_psf(lambda_values, aberration_coeffs, visualize=False)).float().to(device)

    simulated_pre = torch_apply_psf(PSF, pre_tensor)
    if simulated_pre.dim() == 3:
        simulated_pre = simulated_pre.unsqueeze(0)

    K = torch_apply_psf(PSF, ini_tensor)
    if K.dim() == 3:
        K_1 = K.unsqueeze(0)

    criterion = AberrationLoss().to(device)
    loss = criterion(simulated_pre, ini_tensor)
    loss1 = criterion(K_1, ini_tensor)

    return loss.item(), loss1.item()


if __name__ == "__main__":
    main()